{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install langchain tiktoken chromadb python-dotenv ipykernel jupyter arxiv pymupdf\n",
    "!pip3 install sentence_transformers pypdf unstructured\n",
    "!pip3 install auto_gptq\n",
    "!pip install transformers\n",
    "!pip3 install -U langchain-community\n",
    "!pip -qqq install qdrant-client==1.9.1  --progress-bar off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "from chromadb.config import Settings\n",
    "from urllib.error import HTTPError\n",
    "from dataclasses import replace\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import tiktoken # OpenAI's open-source tokenizer\n",
    "import chromadb\n",
    "import logging\n",
    "import random # to sample multiple elements from a list\n",
    "import arxiv\n",
    "import time\n",
    "import os # operating system dependent functionality, to walk through directories and files\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # recursively tries to split by different characters to find one that works\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader # loads pdfs from a given directory\n",
    "from langchain.chains import ConversationalRetrievalChain # looks up relevant documents from the retriever per history and question.\n",
    "from langchain.text_splitter import CharacterTextSplitter # splits the content\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings # wrapper for HuggingFaceBgeEmbeddings models\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.document_loaders import ArxivLoader # loads paper for a given id from Arxiv\n",
    "from langchain.document_loaders import PyPDFLoader # loads a given pdf\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader # loads a given text\n",
    "from langchain.retrievers import ArxivRetriever # loads relevant papers for a given paper id from Arxiv\n",
    "from chromadb.utils import embedding_functions # loads Chroma's embedding functions from OpenAI, HuggingFace, SentenceTransformer and others\n",
    "from langchain.chat_models import ChatOpenAI # wrapper around OpenAI LLMs\n",
    "from langchain.vectorstores import Chroma # wrapper around ChromaDB embeddings platform\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain import HuggingFaceHub # wrapper around HuggingFaceHub models\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "load_dotenv() # loads env variables\n",
    "# logging.basicConfig(level=logging.INFO) # to inspect network behavior and API logic of Arxiv and Chroma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data: Arxiv paper for a given search term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bg-eg\\AppData\\Local\\Temp\\ipykernel_13020\\3958013242.py:8: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in tqdm(search.results()):\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:03,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Paper id 2303.18223v13 with title 'A Survey of Large Language Models' is downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir arxiv_papers\n",
    "dirpath = \"arxiv_papers\"\n",
    "\n",
    "search = arxiv.Search(\n",
    "  query = \"2303.18223\" # ID of the paper A Survey of Large Language Models\n",
    ")\n",
    "\n",
    "for result in tqdm(search.results()):\n",
    "    result.download_pdf(dirpath=dirpath)\n",
    "    print(f\"-> Paper id {result.get_short_id()} with title '{result.title}' is downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = []\n",
    "loader = DirectoryLoader('./arxiv_papers/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "papers = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pages loaded:  131\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of pages loaded: \", len(papers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### load and chunk the paper using chunk size of 500 and overlap of 50,  should definitely experiment with these to find what works best in Our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 10000,\n",
    "    chunk_overlap  = 1000\n",
    ")\n",
    "\n",
    "paper_chunks = text_splitter.split_documents(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='6\\nmultiple reasoning steps, e.g., mathematical word problems.\\nIn contrast, with the chain-of-thought (CoT) prompting\\nstrategy [33], LLMs can solve such tasks by utilizing the\\nprompting mechanism that involves intermediate reasoning\\nsteps for deriving the final answer. This ability is speculated\\nto be potentially obtained by training on code [33, 47]. An\\nempirical study [33] has shown that CoT prompting can\\nbring performance gains (on arithmetic reasoning bench-\\nmarks) when applied to PaLM and LaMDA variants with\\na model size larger than 60B, while its advantage over\\nthe standard prompting becomes more evident when the\\nmodel size exceeds 100B. Furthermore, the performance\\nimprovement with CoT prompting seems to be also varied\\nfor different tasks, e.g., GSM8K >MAWPS >SWAMP for\\nPaLM [33].\\nHow Emergent Abilities Relate to Scaling Laws . In existing\\nliterature [30, 31, 34], scaling laws and emergent abilities\\nprovide two perspectives to understand the advantage of\\nlarge models over small models. In general, scaling law\\n(often measured by language modeling loss ) describes pre-\\ndictable performance relation with the potential effect of\\ndiminishing returns, while emergent abilities (often mea-\\nsured by task performance ) are unpredictable but very prof-\\nitable once such abilities actually emerge. Since the two\\nperspectives reflect different performance trends (continu-\\nous improvement v.s.sharp performance leap), they might\\nlead to misaligned findings or observations. There are also\\nextensive debates on the rationality of emergent abilities.\\nA popular speculation is that emergent abilities might be\\npartially attributed to the evaluation setting for special tasks\\n(e.g., the discontinuous evaluation metrics) [70, 71]: when\\nevaluation metrics are altered accordingly, the sharpness of\\nthe emergent ability curve would disappear. However, the\\nperformance of LLMs on most tasks are perceived by users\\nnaturally in a discontinuous way. For instance, end users\\nprefer a reliable code generated by LLMs that can success-\\nfully pass the test case, but are less interested in selecting a\\nbetter code with fewer errors between two failed ones. More\\nrecently, a study [72] proposes a new evaluation setting\\nthat can enlarge the resolution of task metrics, making task\\nperformance more predictable. Despite these efforts, more\\nfundamental research ( e.g., grokking10) about the working\\nmechanism of LLMs is still in need to understand the emer-\\ngence of certain abilities. The subtle relation between scaling\\nlaw and emergent abilities can be explained by analogy with\\nthe ability acquisition of human11. Take the speaking ability\\nas an example. For children, language development (espe-\\ncially infants) can be also considered as a multi-level process\\nwhere “emergent abilities” occur. Specially, the language\\nability would relatively stable within a time interval, but\\nqualitative change only occurs when evolving into another\\nability level ( e.g., from speaking simple words to speaking\\nsimple sentences). Such a learning process is essentially not\\nsmooth and stable (i.e.,language ability does not develop at\\na constant rate over time), though a child actually grows\\n10. Grokking refers that “a pattern in the data, improving generaliza-\\ntion performance from random chance level to perfect generalization”,\\nquoted from the original paper [73].\\n11. This explanation is only for ease of understanding, and there is\\nnot direct evidence to connect the two points.every day. It is interesting that young parents would be often\\nsurprised by unexpected progress of the speaking ability\\nexhibited by their babies.\\nKey Techniques for LLMs . It has been a long way that\\nLLMs evolve into the current state: general and capable\\nlearners. In the development process, a number of impor-\\ntant techniques are proposed, which largely improve the\\ncapacity of LLMs. Here, we briefly list several important\\ntechniques that (potentially) lead to the success of LLMs, as\\nfollows.\\n•Scaling . As discussed in previous parts, there exists\\nan evident scaling effect in Transformer language mod-\\nels: larger model/data sizes and more training compute\\ntypically lead to an improved model capacity [30, 34]. As\\ntwo representative models, GPT-3 and PaLM explored the\\nscaling limits by increasing the model size to 175B and\\n540B, respectively. Since compute budget is usually limited,\\nscaling laws can be further employed to conduct a more\\ncompute-efficient allocation of the compute resources. For\\nexample, Chinchilla (with more training tokens) outper-\\nforms its counterpart model Gopher (with a larger model\\nsize) by increasing the data scale with the same compute\\nbudget [34]. In addition, data scaling should be with careful\\ncleaning process, since the quality of pre-training data plays\\na key role in the model capacity.\\n•Training . Due to the huge model size, it is very chal-\\nlenging to successfully train a capable LLM. Distributed\\ntraining algorithms are needed to learn the network param-\\neters of LLMs, in which various parallel strategies are of-\\nten jointly utilized. To support distributed training, several\\noptimization frameworks have been released to facilitate\\nthe implementation and deployment of parallel algorithms,\\nsuch as DeepSpeed [74] and Megatron-LM [75–77]. Also, op-\\ntimization tricks are also important for training stability and\\nmodel performance, e.g., restart to overcome training loss\\nspike [56] and mixed precision training [78]. More recently,\\nGPT-4 [46] proposes to develop special infrastructure and\\noptimization methods that reliably predict the performance\\nof large models with much smaller models.\\n•Ability eliciting . After being pre-trained on large-scale\\ncorpora, LLMs are endowed with potential abilities as\\ngeneral-purpose task solvers. These abilities might not be\\nexplicitly exhibited when LLMs perform some specific tasks.\\nAs the technical approach, it is useful to design suitable task\\ninstructions or specific in-context learning strategies to elicit\\nsuch abilities. For instance, chain-of-thought prompting has\\nbeen shown to be useful to solve complex reasoning tasks\\nby including intermediate reasoning steps. Furthermore,\\nwe can perform instruction tuning on LLMs with task\\ndescriptions expressed in natural language, for improving\\nthe generalizability of LLMs on unseen tasks. These eliciting\\ntechniques mainly correspond to the emergent abilities of\\nLLMs, which may not show the same effect on small lan-\\nguage models.\\n•Alignment tuning . Since LLMs are trained to capture\\nthe data characteristics of pre-training corpora (including\\nboth high-quality and low-quality data), they are likely to\\ngenerate toxic, biased, or even harmful content for humans.\\nIt is necessary to align LLMs with human values, e.g., helpful ,\\nhonest , and harmless . For this purpose, InstructGPT [66]', metadata={'source': 'arxiv_papers\\\\2303.18223v13.A_Survey_of_Large_Language_Models.pdf', 'page': 5})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_chunks)\n",
    "paper_chunks[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5770.8702290076335"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_lengths = [len(paper_chunk.page_content) for paper_chunk in paper_chunks]\n",
    "np.average(chunk_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BAAI/bge-base-en\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "embedding_function = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': 'cuda'},\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\bg-eg\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\",write_permission=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3\",temperature=0.0,num_predict=200,repeat_penalty=0.2)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Answer the question and provide additional helpful information,\n",
    "based on the pieces of information, if applicable. Be succinct.\n",
    "\n",
    "Responses should be properly formatted to be easily read.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We Can Also Play With Hugging Face\n",
    "```\n",
    "model_name_or_path = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model_basename = \"gptq_model-4bit-128g\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True,cache_dir=\"D://hugging\",trust_remote_code=True)\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16,cache_dir=\"D://hugging\",trust_remote_code=True,force_download=True, resume_download=True)\n",
    "model.to(\"cuda:0\")\n",
    "```\n",
    "\n",
    "```\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    model_name_or_path,\n",
    "    use_safetensors=True,\n",
    "    trust_remote_code=True,\n",
    "    device=\"cuda:0\",\n",
    "    use_triton=use_triton,\n",
    "    quantize_config=None,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "\n",
    "qdrant = Qdrant.from_documents(\n",
    "    paper_chunks,\n",
    "    embedding_function,\n",
    "    location=\":memory:\",\n",
    "    # path=\"./NEW\",\n",
    "    collection_name=\"document_embeddings\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 41 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "query = \"brief about abdo\"\n",
    "similar_docs = qdrant.similarity_search_with_score(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: Profile\n",
      "•Software Engineer with a focus on artificial intelligence and machine learning. \n",
      "experience proficient in creating, deploying, and managing end-to-end AI projects. I \n",
      "specialize in Agile methodologies and Software Development Life Cycle, ensuring \n",
      "efficient and timely delivery. My expertise extends to Generative AI, where I've led \n",
      "innovative projects pushing the boundaries of traditional problem-solving\n",
      "•Adept at delivering impactful results and contributing to team success in dynamic, \n",
      "collaborative environments. Excited to leverage my expertise in creating and deploying \n",
      "cutting-edge AI solutions to drive innovation in your organization\n",
      "•My dedication to the field of Machine Learning and Artificial Intelligence, along with my \n",
      "problem-solving skills and knowledge of related technologies\n",
      "•I also undertook internships that provided hands-on experience in implementing machine \n",
      "learning algorithms. In one particular internship, I contributed to the development of a \n",
      "recommendation system that significantly improved user engagement. This experience \n",
      "allowed me to apply my theoretical knowledge in a professional setting and exposed me to \n",
      "the intricacies of deploying machine learning models in a production environment.A b d a l r a h m e n  Y o u s e f   Machine Learning\n",
      "abdalrahmenyousif54@gmail.com\n",
      "Egypt , Mansoura\n",
      "LinkedIn\n",
      "abdalrahmen+201097904132\n",
      "Exempted\n",
      "Git\n",
      "\n",
      "score: 0.7553148480826005\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "text: 50\n",
      "TABLE 12: A collection of useful tips for designing prompts that are collected from online notes [453–456] and experiences\n",
      "from our authors, where we also show the related ingredients and principles (introduced in Section 6.1.1). We abbreviate\n",
      "principles as Prin. and list the IDs of the related principles for each prompt. 1⃝: expressing the task goal clearly; 2⃝:\n",
      "decomposing into easy, detailed sub-tasks; 3⃝: providing few-shot demonstrations; 4⃝: utilizing model-friendly format.\n",
      "Ingredient Collected Prompts Prin.\n",
      "Task DescriptionT1. Make your prompt as detailed as possible ,e.g., “Summarize the article into a short paragraph within 50 words. The major\n",
      "storyline and conclusion should be included, and the unimportant details can be omitted. ”1⃝\n",
      "T2. It is helpful to let the LLM know that it is an expert with a prefixed prompt ,e.g., “You are a sophisticated expert in the\n",
      "domain of compute science. ”1⃝\n",
      "T3. Tell the model more what it should do , but not what it should not do. 1⃝\n",
      "T4. To avoid the LLM to generate too long output, you can just use the prompt: “ Question: Short Answer: ”. Besides, you can\n",
      "also use the following suffixes, “ in a or a few words ”, “in one of two sentences ”.1⃝\n",
      "Input DataI1. For the question required factual knowledge, it is useful to first retrieve relevant documents via the search engine, and\n",
      "then concatenate them into the prompt as reference.4⃝\n",
      "I2. To highlight some important parts in your prompt, please use special marks ,e.g., quotation (””) and line break (\\n). You\n",
      "can also use both of them for emphasizing.4⃝\n",
      "Contextual InformationC1. For complex tasks, you can clearly describe the required intermediate steps to accomplish it, e.g., “Please answer the\n",
      "question step by step as: Step 1 - Decompose the question into several sub-questions, ···”2⃝\n",
      "C2. If you want LLMs to provide the score for a text, it is necessary to provide a detailed description about the\n",
      "scoring standard with examples as reference.1⃝\n",
      "C3. When LLMs generate text according to some context ( e.g., making recommendations according to purchase history),\n",
      "instructing them with the explanation about the generated result conditioned on context is helpful to improve the quality\n",
      "of the generated text.2⃝\n",
      "C4. An approach similar to tree-of-thoughts but can be done in one prompt :e.g., Imagine three different experts are answering\n",
      "this question. All experts will write down one step of their thinking, then share it with the group of experts. Then all experts will go on\n",
      "to the next step, etc. If any expert realizes they’re wrong at any point then they leave. The question is2⃝\n",
      "DemonstrationD1.Well-formatted in-context exemplars are very useful, especially for producing the outputs with complex formats. 3⃝\n",
      "D2. For few-shot chain-of-thought prompting, you can also use the prompt “ Let’s think step-by-step ”, and the few-shot\n",
      "examples should be separated by “ \\n”instead of full stop.1⃝3⃝\n",
      "D3. You can also retrieve similar examples in context to supply the useful task-specific knowledge for LLMs. To retrieve\n",
      "more relevant examples, it is useful to first obtain the answer of the question, and then concatenate it with the question for\n",
      "retrieval.3⃝4⃝\n",
      "D4. The diversity of the in-context exemplars within the prompt is also useful. If it is not easy to obtain diverse questions,\n",
      "you can also seek to keep the diversity of the solutions for the questions.3⃝\n",
      "D5. When using chat-based LLMs, you can decompose in-context exemplars into multi-turn messages , to better match the\n",
      "human-chatbot conversation format. Similarly, you can also decompose the reasoning process of an exemplars into multi-turn\n",
      "conversation.3⃝\n",
      "D6.Complex and informative in-context exemplars can help LLMs answer complex questions. 3⃝\n",
      "D7. As a symbol sequence can typically be divided into multiple segments ( e.g.,i1, i2, i3−→i1, i2andi2, i3), the preceding\n",
      "ones can be used as in-context exemplars to guide LLMs to predict the subsequent ones, meanwhile providing historical\n",
      "information.2⃝3⃝\n",
      "D8.Order matters for in-context exemplars and prompts components. For very long input data, the position of the question\n",
      "(first or last) may also affect the performance.3⃝\n",
      "D9. If you can not obtain the in-context exemplars from existing datasets, an alternative way is to use the zero-shot\n",
      "generated ones from the LLM itself.3⃝\n",
      "Other DesignsO1. Let the LLM check its outputs before draw the conclusion, e.g., “Check whether the above solution is correct or not. ” 2⃝\n",
      "O2. If the LLM can not well solve the task, you can seek help from external tools by prompting the LLM to manipulate\n",
      "them. In this way, the tools should be encapsulated into callable APIs with detailed description about their functions, to\n",
      "better guide the LLM to utilize the tools.4⃝\n",
      "O3. The prompt should be self-contained , and better not include pronouns ( e.g., it and they) in the context. 1⃝\n",
      "O4. When using LLMs for comparing two or more examples, the order affects the performance a lot. 1⃝\n",
      "O5. Before the prompt, assigning a role for the LLM is useful to help it better fulfill the following task instruction, e.g., “I\n",
      "want you to act as a lawyer” .1⃝\n",
      "O6. OpenAI models can perform a task better in English than other languages. Thus, it is useful to first\n",
      "translate the input into English and then feed it to LLMs.4⃝\n",
      "O7. For multi-choice questions, it is useful to constrain the output space of the LLM. You can use a more detailed explanation\n",
      "or just imposing constraints on the logits.1⃝\n",
      "O8. For sorting based tasks ( e.g., recommendation), instead of directly outputting the complete text of each item after sorting,\n",
      "one can assign indicators (e.g., ABCD ) to the unsorted items and instruct the LLMs to directly output the sorted indicators.1⃝\n",
      "•For mathematical reasoning tasks, it is more effective to\n",
      "design specific prompts based on the format of programming\n",
      "language. For GSM8k, the designed prompt employs code-\n",
      "formatted few-shot demonstrations to convert this mathe-\n",
      "matical reasoning task into code generation task, which can\n",
      "leverage the strong code synthesis ability of ChatGPT for\n",
      "solving mathematical problems. Further, with the help of an\n",
      "external program executor, we are able to obtain more pre-\n",
      "cise results instead of using LLMs for arithmetic operation.\n",
      "As we can see, the performance is boosted from 78.47 to\n",
      "79.30 on GSM8k, indicating the usefulness of programminglanguage in mathematical reasoning tasks.\n",
      "•In knowledge utilization and complex reasoning tasks,\n",
      "ChatGPT with proper prompts achieves comparable performance\n",
      "or even outperforms the supervised baselines methods. In knowl-\n",
      "edge utilization and complex reasoning tasks, ChatGPT\n",
      "with proper zero-shot or few-shot prompts can achieve\n",
      "comparable performance or even outperform the super-\n",
      "vised methods, e.g., 31.21 (ChatGPT) v.s.34.20 (supervised\n",
      "baseline) on WikiFact. Despite that, ChatGPT still performs\n",
      "worse than supervised baseline models on some specific\n",
      "tasks ( e.g., ARC and WikiFact), since these supervised mod-\n",
      "\n",
      "score: 0.735617937965989\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "text: 104\n",
      "https://doi.org/10.48550/arXiv.2210.02406\n",
      "[441] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K. Lee, and\n",
      "E. Lim, “Plan-and-solve prompting: Improving zero-\n",
      "shot chain-of-thought reasoning by large language\n",
      "models,” CoRR , vol. abs/2305.04091, 2023. [Online].\n",
      "Available: https://doi.org/10.48550/arXiv.2305.04091\n",
      "[442] Q. Lyu, S. Havaldar, A. Stein, L. Zhang, D. Rao,\n",
      "E. Wong, M. Apidianaki, and C. Callison-Burch,\n",
      "“Faithful chain-of-thought reasoning,” CoRR , vol.\n",
      "abs/2301.13379, 2023.\n",
      "[443] L. Gao, A. Madaan, S. Zhou, U. Alon, P . Liu, Y. Yang,\n",
      "J. Callan, and G. Neubig, “PAL: program-aided lan-\n",
      "guage models,” CoRR , vol. abs/2211.10435, 2022.\n",
      "[444] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and\n",
      "Y. Zhuang, “Hugginggpt: Solving ai tasks with chat-\n",
      "gpt and its friends in huggingface,” arXiv preprint\n",
      "arXiv:2303.17580 , 2023.\n",
      "[445] H. Sun, Y. Zhuang, L. Kong, B. Dai, and C. Zhang,\n",
      "“Adaplanner: Adaptive planning from feedback with\n",
      "language models,” arXiv preprint arXiv:2305.16653 ,\n",
      "2023.\n",
      "[446] Y. Lu, P . Lu, Z. Chen, W. Zhu, X. E. Wang, and W. Y.\n",
      "Wang, “Multimodal procedural planning via dual\n",
      "text-image prompting,” CoRR , vol. abs/2305.01795,\n",
      "2023.\n",
      "[447] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang,\n",
      "and Z. Hu, “Reasoning with language model is plan-\n",
      "ning with world model,” CoRR , vol. abs/2305.14992,\n",
      "2023.\n",
      "[448] Z. Chen, K. Zhou, B. Zhang, Z. Gong, W. X. Zhao, and\n",
      "J. Wen, “Chatcot: Tool-augmented chain-of-thought\n",
      "reasoning on chat-based large language models,”\n",
      "CoRR , vol. abs/2305.14323, 2023.\n",
      "[449] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran,\n",
      "K. Narasimhan, and Y. Cao, “React: Synergizing rea-\n",
      "soning and acting in language models,” CoRR , vol.\n",
      "abs/2210.03629, 2022.\n",
      "[450] N. Shinn, F. Cassano, B. Labash, A. Gopinath,\n",
      "K. Narasimhan, and S. Yao, “Reflexion: Language\n",
      "agents with verbal reinforcement learning,” 2023.\n",
      "[451] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao,\n",
      "and K. Narasimhan, “Tree of thoughts: Deliberate\n",
      "problem solving with large language models,” CoRR ,\n",
      "vol. abs/2305.10601, 2023.\n",
      "[452] V . Liu and L. B. Chilton, “Design guidelines for\n",
      "prompt engineering text-to-image generative mod-\n",
      "els,” in Proceedings of the 2022 CHI Conference on Human\n",
      "Factors in Computing Systems , 2022, pp. 1–23.\n",
      "[453] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea,\n",
      "H. Gilbert, A. Elnashar, J. Spencer-Smith, and D. C.\n",
      "Schmidt, “A prompt pattern catalog to enhance\n",
      "prompt engineering with chatgpt,” arXiv preprint\n",
      "arXiv:2302.11382 , 2023.\n",
      "[454] S. K. K. Santu and D. Feng, “Teler: A general\n",
      "taxonomy of LLM prompts for benchmarking\n",
      "complex tasks,” CoRR , vol. abs/2305.11430, 2023.\n",
      "[Online]. Available: https://doi.org/10.48550/arXiv.\n",
      "2305.11430\n",
      "[455] OpenAI, “Gpt best practices,” OpenAI , 2023.\n",
      "[Online]. Available: https://platform.openai.com/\n",
      "docs/guides/gpt-best-practices[456] Contributors, “Ai short,” 2023. [Online]. Available:\n",
      "https://www.aishort.top/\n",
      "[457] ——, “Awesome chatgpt prompts,” Github , 2023.\n",
      "[Online]. Available: https://github.com/f/awesome-\n",
      "chatgpt-prompts/\n",
      "[458] J. Jiang, K. Zhou, Z. Dong, K. Ye, W. X. Zhao, and\n",
      "J. Wen, “Structgpt: A general framework for large lan-\n",
      "guage model to reason over structured data,” CoRR ,\n",
      "vol. abs/2305.09645, 2023.\n",
      "[459] L. Beurer-Kellner, M. Fischer, and M. Vechev,\n",
      "“Prompting is programming: A query language for\n",
      "large language models,” Proceedings of the ACM on\n",
      "Programming Languages , vol. 7, no. PLDI, pp. 1946–\n",
      "1969, 2023.\n",
      "[460] P . Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N.\n",
      "Wu, S.-C. Zhu, and J. Gao, “Chameleon: Plug-and-\n",
      "play compositional reasoning with large language\n",
      "models,” arXiv preprint arXiv:2304.09842 , 2023.\n",
      "[461] R. Ren, Y. Wang, Y. Qu, W. X. Zhao, J. Liu, H. Tian,\n",
      "H. Wu, J.-R. Wen, and H. Wang, “Investigating\n",
      "the factual knowledge boundary of large language\n",
      "models with retrieval augmentation,” arXiv preprint\n",
      "arXiv:2307.11019 , 2023.\n",
      "[462] Y. Hou, J. Zhang, Z. Lin, H. Lu, R. Xie, J. J. McAuley,\n",
      "and W. X. Zhao, “Large language models are zero-\n",
      "shot rankers for recommender systems,” CoRR , vol.\n",
      "abs/2305.08845, 2023.\n",
      "[463] S. Chang and E. Fosler-Lussier, “How to prompt\n",
      "llms for text-to-sql: A study in zero-shot, single-\n",
      "domain, and cross-domain settings,” CoRR , vol.\n",
      "abs/2305.11853, 2023. [Online]. Available: https:\n",
      "//doi.org/10.48550/arXiv.2305.11853\n",
      "[464] Y. Wen, N. Jain, J. Kirchenbauer, M. Goldblum,\n",
      "J. Geiping, and T. Goldstein, “Hard prompts\n",
      "made easy: Gradient-based discrete optimization\n",
      "for prompt tuning and discovery,” CoRR , vol.\n",
      "abs/2302.03668, 2023. [Online]. Available: https:\n",
      "//doi.org/10.48550/arXiv.2302.03668\n",
      "[465] T. Gao, A. Fisch, and D. Chen, “Making pre-trained\n",
      "language models better few-shot learners,” in Proceed-\n",
      "ings of the 59th Annual Meeting of the Association for\n",
      "Computational Linguistics and the 11th International Joint\n",
      "Conference on Natural Language Processing, ACL/IJCNLP\n",
      "2021, (Volume 1: Long Papers), Virtual Event, August 1-\n",
      "6, 2021 , C. Zong, F. Xia, W. Li, and R. Navigli, Eds.\n",
      "Association for Computational Linguistics, 2021, pp.\n",
      "3816–3830.\n",
      "[466] L. Chen, J. Chen, T. Goldstein, H. Huang, and T. Zhou,\n",
      "“Instructzero: Efficient instruction optimization for\n",
      "black-box large language models,” CoRR , vol.\n",
      "abs/2306.03082, 2023. [Online]. Available: https:\n",
      "//doi.org/10.48550/arXiv.2306.03082\n",
      "[467] M. Deng, J. Wang, C. Hsieh, Y. Wang, H. Guo, T. Shu,\n",
      "M. Song, E. P . Xing, and Z. Hu, “Rlprompt: Optimiz-\n",
      "ing discrete text prompts with reinforcement learn-\n",
      "ing,” in Proceedings of the 2022 Conference on Empirical\n",
      "Methods in Natural Language Processing, EMNLP 2022,\n",
      "Abu Dhabi, United Arab Emirates, December 7-11, 2022 ,\n",
      "Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds. Asso-\n",
      "ciation for Computational Linguistics, 2022, pp. 3369–\n",
      "3391.\n",
      "\n",
      "score: 0.734512585766191\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "text: 106\n",
      "Learning Representations, ICLR 2022, Virtual Event, April\n",
      "25-29, 2022 , 2022.\n",
      "[489] Z. Wu, Y. Wang, J. Ye, and L. Kong, “Self-adaptive in-\n",
      "context learning,” CoRR , vol. abs/2212.10375, 2022.\n",
      "[490] Y. Gu, L. Dong, F. Wei, and M. Huang, “Pre-training\n",
      "to learn in context,” CoRR , vol. abs/2305.09137, 2023.\n",
      "[491] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi,\n",
      "“Metaicl: Learning to learn in context,” in Proceedings\n",
      "of the 2022 Conference of the North American Chapter\n",
      "of the Association for Computational Linguistics: Human\n",
      "Language Technologies, NAACL 2022, Seattle, WA, United\n",
      "States, July 10-15, 2022 , M. Carpuat, M. de Marneffe,\n",
      "and I. V . M. Ru ´ız, Eds., 2022, pp. 2791–2809.\n",
      "[492] M. Hahn and N. Goyal, “A theory of emergent\n",
      "in-context learning as implicit structure induction,”\n",
      "CoRR , vol. abs/2303.07971, 2023.\n",
      "[493] J. Pan, T. Gao, H. Chen, and D. Chen, “What in-context\n",
      "learning ”learns” in-context: Disentangling task recog-\n",
      "nition and task learning,” CoRR , vol. abs/2305.09731,\n",
      "2023.\n",
      "[494] N. Wies, Y. Levine, and A. Shashua, “The learnability\n",
      "of in-context learning,” CoRR , vol. abs/2303.07895,\n",
      "2023.\n",
      "[495] A. Webson and E. Pavlick, “Do prompt-based models\n",
      "really understand the meaning of their prompts?” in\n",
      "Proceedings of the 2022 Conference of the North American\n",
      "Chapter of the Association for Computational Linguistics:\n",
      "Human Language Technologies, NAACL 2022, Seattle,\n",
      "WA, United States, July 10-15, 2022 , 2022, pp. 2300–\n",
      "2344.\n",
      "[496] J. von Oswald, E. Niklasson, E. Randazzo, J. Sacra-\n",
      "mento, A. Mordvintsev, A. Zhmoginov, and M. Vla-\n",
      "dymyrov, “Transformers learn in-context by gradient\n",
      "descent,” CoRR , vol. abs/2212.07677, 2022.\n",
      "[497] C. Olsson, N. Elhage, N. Nanda, N. Joseph,\n",
      "N. DasSarma, T. Henighan, B. Mann, A. Askell,\n",
      "Y. Bai, A. Chen, T. Conerly, D. Drain, D. Gan-\n",
      "guli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston,\n",
      "A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei,\n",
      "T. Brown, J. Clark, J. Kaplan, S. McCandlish, and\n",
      "C. Olah, “In-context learning and induction heads,”\n",
      "CoRR , vol. abs/2209.11895, 2022.\n",
      "[498] E. Aky ¨urek, D. Schuurmans, J. Andreas, T. Ma, and\n",
      "D. Zhou, “What learning algorithm is in-context learn-\n",
      "ing? investigations with linear models,” CoRR , vol.\n",
      "abs/2211.15661, 2022.\n",
      "[499] J. Wei, J. Wei, Y. Tay, D. Tran, A. Webson, Y. Lu,\n",
      "X. Chen, H. Liu, D. Huang, D. Zhou et al. , “Larger\n",
      "language models do in-context learning differently,”\n",
      "arXiv preprint arXiv:2303.03846 , 2023.\n",
      "[500] J. Coda-Forno, M. Binz, Z. Akata, M. M. Botvinick,\n",
      "J. X. Wang, and E. Schulz, “Meta-in-context learning\n",
      "in large language models,” CoRR , vol. abs/2305.12907,\n",
      "2023.\n",
      "[501] J. W. Wei, L. Hou, A. K. Lampinen, X. Chen, D. Huang,\n",
      "Y. Tay, X. Chen, Y. Lu, D. Zhou, T. Ma, and Q. V .\n",
      "Le, “Symbol tuning improves in-context learning in\n",
      "language models,” CoRR , vol. abs/2305.08298, 2023.\n",
      "[502] Z. Chu, J. Chen, Q. Chen, W. Yu, T. He, H. Wang,\n",
      "W. Peng, M. Liu, B. Qin, and T. Liu, “A survey of\n",
      "chain of thought reasoning: Advances, frontiers andfuture,” CoRR , vol. abs/2309.15402, 2023.\n",
      "[503] S. Miao, C. Liang, and K. Su, “A diverse corpus\n",
      "for evaluating and developing english math word\n",
      "problem solvers,” in Proceedings of the 58th Annual\n",
      "Meeting of the Association for Computational Linguistics,\n",
      "ACL 2020, Online, July 5-10, 2020 , D. Jurafsky, J. Chai,\n",
      "N. Schluter, and J. R. Tetreault, Eds. Association for\n",
      "Computational Linguistics, 2020, pp. 975–984.\n",
      "[504] A. Talmor, J. Herzig, N. Lourie, and J. Berant, “Com-\n",
      "monsenseqa: A question answering challenge tar-\n",
      "geting commonsense knowledge,” in Proceedings of\n",
      "the 2019 Conference of the North American Chapter of\n",
      "the Association for Computational Linguistics: Human\n",
      "Language Technologies, NAACL-HLT 2019, Minneapolis,\n",
      "MN, USA, June 2-7, 2019, Volume 1 (Long and Short\n",
      "Papers) , J. Burstein, C. Doran, and T. Solorio, Eds.\n",
      "Association for Computational Linguistics, 2019, pp.\n",
      "4149–4158.\n",
      "[505] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwa-\n",
      "sawa, “Large language models are zero-shot reason-\n",
      "ers,” CoRR , vol. abs/2205.11916, 2022.\n",
      "[506] W. Chen, X. Ma, X. Wang, and W. W. Cohen, “Program\n",
      "of thoughts prompting: Disentangling computation\n",
      "from reasoning for numerical reasoning tasks,” CoRR ,\n",
      "vol. abs/2211.12588, 2022.\n",
      "[507] L. Gao, A. Madaan, S. Zhou, U. Alon, P . Liu, Y. Yang,\n",
      "J. Callan, and G. Neubig, “PAL: program-aided lan-\n",
      "guage models,” in International Conference on Machine\n",
      "Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii,\n",
      "USA , A. Krause, E. Brunskill, K. Cho, B. Engelhardt,\n",
      "S. Sabato, and J. Scarlett, Eds., 2023.\n",
      "[508] X. Zhao, Y. Xie, K. Kawaguchi, J. He, and Q. Xie, “Au-\n",
      "tomatic model selection with large language models\n",
      "for reasoning,” CoRR , vol. abs/2305.14333, 2023.\n",
      "[509] Y. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J.-G. Lou,\n",
      "and W. Chen, “Making large language models better\n",
      "reasoners with step-aware verifier,” 2023.\n",
      "[510] O. Yoran, T. Wolfson, B. Bogin, U. Katz, D. Deutch,\n",
      "and J. Berant, “Answering questions by meta-\n",
      "reasoning over multiple chains of thought,” CoRR ,\n",
      "vol. abs/2304.13007, 2023.\n",
      "[511] Z. Ling, Y. Fang, X. Li, Z. Huang, M. Lee, R. Memi-\n",
      "sevic, and H. Su, “Deductive verification of chain-of-\n",
      "thought reasoning,” CoRR , vol. abs/2306.03872, 2023.\n",
      "[512] T. Xue, Z. Wang, Z. Wang, C. Han, P . Yu, and H. Ji,\n",
      "“RCOT: detecting and rectifying factual inconsistency\n",
      "in reasoning by reversing chain-of-thought,” CoRR ,\n",
      "vol. abs/2305.11499, 2023.\n",
      "[513] Y. Weng, M. Zhu, F. Xia, B. Li, S. He, K. Liu, and\n",
      "J. Zhao, “Large language models are better reasoners\n",
      "with self-verification,” CoRR, abs/2212.09561 , 2023.\n",
      "[514] W. Jiang, H. Shi, L. Yu, Z. Liu, Y. Zhang, Z. Li, and\n",
      "J. T. Kwok, “Forward-backward reasoning in large\n",
      "language models for mathematical verification,” 2023.\n",
      "[515] J. Long, “Large language model guided tree-of-\n",
      "thought,” CoRR , vol. abs/2305.08291, 2023.\n",
      "[516] S. Mo and M. Xin, “Tree of uncertain thoughts\n",
      "reasoning for large language models,” CoRR , vol.\n",
      "abs/2309.07694, 2023.\n",
      "[517] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger,\n",
      "L. Gianinazzi, J. Gajda, T. Lehmann, M. Podstawski,\n",
      "\n",
      "score: 0.7333931274903357\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for doc, score in similar_docs:\n",
    "    print(f\"text: {doc.page_content}\\n\")\n",
    "    print(f\"score: {score}\")\n",
    "    print(\"-\" * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 156 ms\n",
      "Wall time: 132 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "retriever = qdrant.as_retriever(search_kwargs={\"k\": 1})\n",
    "retrieved_docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 98ee58aa3dfc4bee8ade9699579d8357\n",
      "\n",
      "text: learning algorithms. In one particular internship, I contributed to the development of a \n",
      "recommendation system that significantly improved user engagement. This experience \n",
      "allowed me to apply my theoretical knowledge in a professional setting and exposed me to \n",
      "the intricacies of deploying machine learning models in a production environment.A b d a l r a h m e n  Y o u s e f   Machine Learning\n",
      "abdalrahmenyousif54@gmail.com\n",
      "Egypt , Mansoura\n",
      "LinkedIn\n",
      "abdalrahmen+201097904132\n",
      "Exempted\n",
      "Git\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"id: {doc.metadata['_id']}\\n\")\n",
    "    print(f\"text: {doc.page_content}\\n\")\n",
    "    print(\"-\" * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                  chain_type=\"stuff\",\n",
    "                                  retriever=retriever,chain_type_kwargs={\"prompt\": prompt, \"verbose\": True},\n",
    "                                  return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Use the following pieces of information to answer the user's question.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Context: collaborative environments. Excited to leverage my expertise in creating and deploying \n",
      "cutting-edge AI solutions to drive innovation in your organization\n",
      "•My dedication to the field of Machine Learning and Artificial Intelligence, along with my \n",
      "problem-solving skills and knowledge of related technologies\n",
      "•I also undertook internships that provided hands-on experience in implementing machine \n",
      "learning algorithms. In one particular internship, I contributed to the development of a \n",
      "recommendation system that significantly improved user engagement. This experience \n",
      "allowed me to apply my theoretical knowledge in a professional setting and exposed me to \n",
      "the intricacies of deploying machine learning models in a production environment.A b d a l r a h m e n  Y o u s e f   Machine Learning\n",
      "abdalrahmenyousif54@gmail.com\n",
      "Egypt , Mansoura\n",
      "LinkedIn\n",
      "abdalrahmen+201097904132\n",
      "Exempted\n",
      "Git\n",
      "\n",
      "My experience with Reinforcement Learning has not only \n",
      "enhanced my technical skills but also provided me with a deep \n",
      "understanding of how intelligent agents can learn to make \n",
      "optimal decisions in dynamic and uncertain environments. I am \n",
      "passionate about leveraging RL techniques to solve real-world \n",
      "problems and drive innovations in AI-driven applications \n",
      "(Autonomous Driving &G ame Playing & Automated Trading \n",
      "Strategies )\n",
      "AskY, \n",
      "https://github.com/abdalrahmenyousifMohamed/LLM/tree/2185\n",
      "147ea69a9c684582aeb46aff439988766102/ClassGPT\n",
      "Ask Your FilesJuly 2023 – August 2023\n",
      "Car Plate Characters Recognition, \n",
      "Car Plate Characters Recognition with YOLO\n",
      "Led a comprehensive exploration to develop a robust \n",
      "machine learning model for Arabic letter and number \n",
      "recognition on Egyptian car plates\n",
      "Question: who is abdalrahmen\n",
      "\n",
      "Answer the question and provide additional helpful information,\n",
      "based on the pieces of information, if applicable. Be succinct.\n",
      "\n",
      "Responses should be properly formatted to be easily read.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "CPU times: total: 125 ms\n",
      "Wall time: 13min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = qa.invoke(\"who is abdalrahmen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided information, Abdalrahmen Yousif is a professional with expertise in creating and deploying cutting-edge AI solutions, specifically in Machine Learning and Artificial Intelligence. He has experience in implementing machine learning algorithms, including Reinforcement Learning"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'arxiv_papers\\\\Abdalrahmen (1).pdf',\n",
       "  'page': 0,\n",
       "  '_id': 'dbd3dc01b6ac40d696897ef3b2ccddd7',\n",
       "  '_collection_name': 'document_embeddings'},\n",
       " {'source': 'arxiv_papers\\\\Abdalrahmen (1).pdf',\n",
       "  'page': 1,\n",
       "  '_id': 'eb0e236f3b524df6aa3509621f9f440c',\n",
       "  '_collection_name': 'document_embeddings'},\n",
       " {'source': 'arxiv_papers\\\\2303.18223v13.A_Survey_of_Large_Language_Models.pdf',\n",
       "  'page': 61,\n",
       "  '_id': '8e957e5a4ac64b02b53add8f694b5a8c',\n",
       "  '_collection_name': 'document_embeddings'},\n",
       " {'source': 'arxiv_papers\\\\2303.18223v13.A_Survey_of_Large_Language_Models.pdf',\n",
       "  'page': 11,\n",
       "  '_id': 'be8d01804cd0476bb8c1c62450fe0afb',\n",
       "  '_collection_name': 'document_embeddings'},\n",
       " {'source': 'arxiv_papers\\\\2303.18223v13.A_Survey_of_Large_Language_Models.pdf',\n",
       "  'page': 10,\n",
       "  '_id': '30d1b06587e84689bb754e24e4c797ae',\n",
       "  '_collection_name': 'document_embeddings'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[source.metadata for source in response[\"source_documents\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
